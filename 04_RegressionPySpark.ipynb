{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e94809fa-78f3-4e0b-88ff-42016c9b6df1",
   "metadata": {},
   "source": [
    "Machine learning avec Pyspark\n",
    "========\n",
    "Régression avec PySpark\n",
    "--------\n",
    "- - - -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975bef30-0709-4347-8802-5ea057a3f9f8",
   "metadata": {
    "tags": []
   },
   "source": [
    "Spark ML est un module très récent, développé en parallèle par Databricks et UC Berkeley AMPLab et lancé fin 2015. Spark ML permet d'exécuter la majorité des algorithmes de Machine Learning de façon distribuée pour un très grand gain en performance.\n",
    "\n",
    "Dans cet exercice, nous étudierons le cas d'une régression simple de façon à comprendre comment préparer les données et faire face à un problème de Machine Learning grâce à Spark ML. Des algorithmes plus poussés font l'objectif de l'exercice suivant.\n",
    "\n",
    "* __(a)__ Exécuter la cellule ci-dessous pour construire une SparkSession pour notre exercice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3a30ea0-571b-4db5-89c0-9e9e58d12f18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://173144cdf556:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f48d3081c40>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import de SparkSession et SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "\n",
    "# Définition d'un SparkContext en local\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "# Construction d'une session Spark\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Introduction à Spark ML\") \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663e1864-5016-4dd6-b76a-33bcc98ee114",
   "metadata": {},
   "source": [
    "## 1. Importation de la base de données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2dfc48-f736-46f7-b8bf-2372209e9abc",
   "metadata": {},
   "source": [
    "Dans cet exercice, la base de données utilisée est Year Prediction MSD (https://archive.ics.uci.edu/ml/datasets/YearPredictionMSD). Elle contient des caractéristiques audio de 515345 chansons parues entre 1922 et 2011. Ces chansons sont essentiellement des tubes commerciaux occidentaux.\n",
    "\n",
    "Cette base de données contient 91 variables :\n",
    "* Une variable contenant l'année de la chanson.\n",
    "* 12 variables contenant une projection à 12 dimensions du timbre audio de la chanson.\n",
    "* 78 variables contenant des informations de covariance du timbre audio.\n",
    "\n",
    "L'objectif est d'estimer l'année de sortie d'une chanson en fonction de ses caractéristiques audio. Pour cela nous allons implémenter une régression linéaire simple sur les informations du timbre pour prédire l'année de sortie.\n",
    "\n",
    "* __(a)__ Charger le fichier YearPredictionMSD.txt dans un DataFrame nommé df_raw.\n",
    "* __(b)__ Afficher un extrait de la base de données avec une méthode de votre choix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "279ee391-5b8e-4f1c-ac7a-a085a3678cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n",
      "| _c0| _c1| _c2| _c3| _c4| _c5| _c6| _c7| _c8| _c9|_c10|_c11|_c12|_c13|_c14|_c15|_c16|_c17|_c18|_c19|_c20|_c21|_c22|_c23|_c24|_c25|_c26|_c27|_c28|_c29|_c30|_c31|_c32|_c33|_c34|_c35|_c36|_c37|_c38|_c39|_c40|_c41|_c42|_c43|_c44|_c45|_c46|_c47|_c48|_c49|_c50|_c51|_c52|_c53|_c54|_c55|_c56|_c57|_c58|_c59|_c60|_c61|_c62|_c63|_c64|_c65|_c66|_c67|_c68|_c69|_c70|_c71|_c72|_c73|_c74|_c75|_c76|_c77|_c78|_c79|_c80|_c81|_c82|_c83|_c84|_c85|_c86|_c87|_c88|_c89|_c90|\n",
      "+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n",
      "|2001|4...|2...|7...|8...|-...|-...|-...|-...|7...|-...|3...|-...|1...|6...|9...|6...|4...|3...|3...|2...|2...|1...|1...|1...|-...|-...|9...|4...|-...|-...|1...|3...|1...|1...|7...|-...|-...|-...|-...|7...|-...|-...|-...|-...|1...|-...|8...|2...|-...|3...|-...|-...|1...|4...|-...|-...|1...|6...|2...|-...|-...|7...|-...|-...|-...|-...|4...|7...|2...|6...|-...|-...|-...|7...|-...|-...|-...|1...|-...|4...|1...|-...|5...|1...|1...|-...|6...|-...|-...|2...|\n",
      "|2001|4...|1...|7...|1...|-...|-...|8...|-...|1...|4...|2...|0...|4...|2...|6...|4...|7...|4...|7...|3...|3...|2...|1...|3...|1...|-...|1...|1...|-...|-...|1...|4...|3...|-...|4...|2...|-...|-...|-...|2...|-...|1...|-...|-...|-...|-...|-...|1...|1...|4...|2...|1...|7...|-...|-...|1...|7...|-...|-...|-...|-...|-...|1...|-...|4...|1...|-...|4...|4...|-...|4...|1...|1...|-...|1...|-...|-...|1...|-...|-...|5...|-...|3...|4...|-...|-...|7...|1...|5...|2...|\n",
      "+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Chargement du fichier \" YearPredictionMSD.txt\" dans un DataFrame\n",
    "df_raw = spark.read.csv('data/YearPredictionMSD.txt')\n",
    "\n",
    "# Première méthode d'affichage\n",
    "df_raw.show(2, truncate = 4)\n",
    "# Modifier les valeurs de 'truncate' ne permet pas de bien visualiser les données\n",
    "# à cause du nombre de variables\n",
    "\n",
    "# Deuxième méthode d'affichage\n",
    "# df_raw.sample(False, .00001, seed = 222).toPandas()\n",
    "# Utiliser toPandas permet de mieux visualiser les données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad656505-b079-4613-b99a-6991e6b920ba",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "Afficher une base données avec la méthode show() est plus rapide. Cependant, cet affichage peut parfois être incompréhensible lorsqu'il y a trop de variables.<br>\n",
    "On peut alors sélectionner quelques variables et tronquer l'affichage pour le rendre propre ou alors privilégier la succession des méthodes sample() et toPandas() en faisant attention à choisir un nombre raisonnable de lignes à afficher. Il faut bien garder en tête que même si la méthode sample() est relativement rapide, elle passe néanmoins par un décompte du nombre de ligne, une opération simple mais qui n'est pas rapide en Spark.\n",
    "</div>\n",
    "\n",
    "L'exercice précédent montre que le parsing effectué par PySpark a tendance à enregistrer toutes les variables en string même lorsqu'elles sont numériques. La vérification de cette information peut s'effectuer en utilisant la méthode printSchema() :\n",
    "\n",
    "```python\n",
    "df_raw.printSchema()\n",
    "```\n",
    "\n",
    "Pour modifier le type de chacune des variables, il faudrait changer son type comme suit :\n",
    "\n",
    "```python\n",
    "df_raw.select(df_raw._c0.cast(\"double\"),\n",
    "                  df_raw._c1.cast(\"double\"),\n",
    "                  df_raw._c2.cast(\"double\"),\n",
    "                  df_raw._c3.cast(\"double\"),\n",
    "                  ...)\n",
    "```\n",
    "\n",
    "Une telle tâche devient de plus en plus fastidieuse quand le nombre de variables devient important. Cette démarche peut être automatisée grâce à la fonction col issue du sous-module pyspark.sql.functions. La fonction col permet de nommer directement une colonne et d'automatiser ce type de démarche au sein d'une boucle. Les deux lignes suivantes permettent alors de changer toutes les colonnes en double dans un nouveau DataFrame df :\n",
    "\n",
    "```python\n",
    "exprs = [col(c).cast(\"double\") for c in df_raw.columns]\n",
    "df = df_raw.select(*exprs)\n",
    "```\n",
    "\n",
    "* __(c)__ Importer la fonction col du sous-module pyspark.sql.functions.\n",
    "* __(d)__ Créer un DataFrame df à partir de df_raw en changeant les types des colonnes relatives au timbre en double et l'année en int.\n",
    "* __(e)__ Afficher le schéma des variables du df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "052fc9ec-a297-4363-89ad-54d182d4d9d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- _c1: double (nullable = true)\n",
      " |-- _c2: double (nullable = true)\n",
      " |-- _c3: double (nullable = true)\n",
      " |-- _c4: double (nullable = true)\n",
      " |-- _c5: double (nullable = true)\n",
      " |-- _c6: double (nullable = true)\n",
      " |-- _c7: double (nullable = true)\n",
      " |-- _c8: double (nullable = true)\n",
      " |-- _c9: double (nullable = true)\n",
      " |-- _c10: double (nullable = true)\n",
      " |-- _c11: double (nullable = true)\n",
      " |-- _c12: double (nullable = true)\n",
      " |-- _c13: double (nullable = true)\n",
      " |-- _c14: double (nullable = true)\n",
      " |-- _c15: double (nullable = true)\n",
      " |-- _c16: double (nullable = true)\n",
      " |-- _c17: double (nullable = true)\n",
      " |-- _c18: double (nullable = true)\n",
      " |-- _c19: double (nullable = true)\n",
      " |-- _c20: double (nullable = true)\n",
      " |-- _c21: double (nullable = true)\n",
      " |-- _c22: double (nullable = true)\n",
      " |-- _c23: double (nullable = true)\n",
      " |-- _c24: double (nullable = true)\n",
      " |-- _c25: double (nullable = true)\n",
      " |-- _c26: double (nullable = true)\n",
      " |-- _c27: double (nullable = true)\n",
      " |-- _c28: double (nullable = true)\n",
      " |-- _c29: double (nullable = true)\n",
      " |-- _c30: double (nullable = true)\n",
      " |-- _c31: double (nullable = true)\n",
      " |-- _c32: double (nullable = true)\n",
      " |-- _c33: double (nullable = true)\n",
      " |-- _c34: double (nullable = true)\n",
      " |-- _c35: double (nullable = true)\n",
      " |-- _c36: double (nullable = true)\n",
      " |-- _c37: double (nullable = true)\n",
      " |-- _c38: double (nullable = true)\n",
      " |-- _c39: double (nullable = true)\n",
      " |-- _c40: double (nullable = true)\n",
      " |-- _c41: double (nullable = true)\n",
      " |-- _c42: double (nullable = true)\n",
      " |-- _c43: double (nullable = true)\n",
      " |-- _c44: double (nullable = true)\n",
      " |-- _c45: double (nullable = true)\n",
      " |-- _c46: double (nullable = true)\n",
      " |-- _c47: double (nullable = true)\n",
      " |-- _c48: double (nullable = true)\n",
      " |-- _c49: double (nullable = true)\n",
      " |-- _c50: double (nullable = true)\n",
      " |-- _c51: double (nullable = true)\n",
      " |-- _c52: double (nullable = true)\n",
      " |-- _c53: double (nullable = true)\n",
      " |-- _c54: double (nullable = true)\n",
      " |-- _c55: double (nullable = true)\n",
      " |-- _c56: double (nullable = true)\n",
      " |-- _c57: double (nullable = true)\n",
      " |-- _c58: double (nullable = true)\n",
      " |-- _c59: double (nullable = true)\n",
      " |-- _c60: double (nullable = true)\n",
      " |-- _c61: double (nullable = true)\n",
      " |-- _c62: double (nullable = true)\n",
      " |-- _c63: double (nullable = true)\n",
      " |-- _c64: double (nullable = true)\n",
      " |-- _c65: double (nullable = true)\n",
      " |-- _c66: double (nullable = true)\n",
      " |-- _c67: double (nullable = true)\n",
      " |-- _c68: double (nullable = true)\n",
      " |-- _c69: double (nullable = true)\n",
      " |-- _c70: double (nullable = true)\n",
      " |-- _c71: double (nullable = true)\n",
      " |-- _c72: double (nullable = true)\n",
      " |-- _c73: double (nullable = true)\n",
      " |-- _c74: double (nullable = true)\n",
      " |-- _c75: double (nullable = true)\n",
      " |-- _c76: double (nullable = true)\n",
      " |-- _c77: double (nullable = true)\n",
      " |-- _c78: double (nullable = true)\n",
      " |-- _c79: double (nullable = true)\n",
      " |-- _c80: double (nullable = true)\n",
      " |-- _c81: double (nullable = true)\n",
      " |-- _c82: double (nullable = true)\n",
      " |-- _c83: double (nullable = true)\n",
      " |-- _c84: double (nullable = true)\n",
      " |-- _c85: double (nullable = true)\n",
      " |-- _c86: double (nullable = true)\n",
      " |-- _c87: double (nullable = true)\n",
      " |-- _c88: double (nullable = true)\n",
      " |-- _c89: double (nullable = true)\n",
      " |-- _c90: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "exprs = [col(c).cast(\"double\") for c in df_raw.columns[1:91]]\n",
    "df = df_raw.select(df_raw._c0.cast(\"int\"), *exprs )\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62454755-3a30-46ac-b322-552082cbe8d0",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "Inférer le bon type des variables peut paraître superflu et certains algorithmes fonctionnent même lorsque les variables numériques sont de type string. Cependant, il s'agit d'une mesure de sécurité importante car cela peut entraîner beaucoup de bugs potentiels.<br>\n",
    "Une deuxième mesure de sécurité à prendre en compte est de supprimer ou remplacer les valeurs manquantes. La base de données est ici dépourvue de valeurs manquantes.\n",
    "</div>\n",
    "\n",
    "* __(g)__ Afficher un résumé descriptif de la base de données df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe793b2-3c54-4828-a929-b38fe896d5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage d'un résumé descriptif des données\n",
    "df.describe().toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb185e8-0f85-432f-8c54-b55fe874af45",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "Une bonne pratique est de mettre la variable à prédire en première position.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b11130b-9f9d-42ed-940b-3d3f4663e1cb",
   "metadata": {},
   "source": [
    "## 2. Mise en forme de la base en format svmlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb74454-29b6-4c10-8e65-47a45cc76b87",
   "metadata": {},
   "source": [
    "Pour pouvoir être utilisée par les algorithmes de Machine Learning de Spark ML, la base de données doit être un DataFrame contenant 2 colonnes :\n",
    "* La colonne label contenant la variable à prédire (label en anglais).\n",
    "* La colonne features contenant les variables explicatives (features en anglais).\n",
    "\n",
    "La fonction DenseVector() issue du package pyspark.ml.linalg permet de regrouper plusieurs variables en une seule variable.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "Pour pouvoir utiliser la fonction DenseVector(), il faut utiliser la méthode map après avoir transformer le DataFrame en rdd.\n",
    "</div>\n",
    "\n",
    "__Exemple :__\n",
    "\n",
    "```python\n",
    "rdd_ml = df.rdd.map(lambda x: (x[0], DenseVector(x[1:]))) # en supposant que la variable à expliquer est en première position\n",
    "```\n",
    "\n",
    "__Exemple :__\n",
    "\n",
    "```python\n",
    "df_ml = spark.createDataFrame(rdd_ml, ['label', 'features'])\n",
    "```\n",
    "\n",
    "* __(a)__ Importer la fonction DenseVector du package pyspark.ml.linalg.\n",
    "* __(b)__ Créer un rdd rdd_ml séparant la variable à expliquer des features (à mettre sous forme DenseVector).\n",
    "* __(c)__ Créer un DataFrame df_ml contenant notre base de données sous deux variables : 'label' et 'features'.\n",
    "* __(d)__ Afficher un extrait de df_ml."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49186000-9c89-480c-bf99-0a52232dd5c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "| 2001|[49.94357,21.4711...|\n",
      "| 2001|[48.73215,18.4293...|\n",
      "| 2001|[50.95714,31.8560...|\n",
      "| 2001|[48.2475,-1.89837...|\n",
      "| 2001|[50.9702,42.20998...|\n",
      "| 2001|[50.54767,0.31568...|\n",
      "| 2001|[50.57546,33.1784...|\n",
      "| 2001|[48.26892,8.97526...|\n",
      "| 2001|[49.75468,33.9958...|\n",
      "| 2007|[45.17809,46.3423...|\n",
      "+-----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import de DenseVector du package pyspark.ml.linalg\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "\n",
    "# Création d'un rdd en séparant la variable à expliquer des features\n",
    "rdd_ml = df.rdd.map(lambda x: (x[0], DenseVector(x[1:])))\n",
    "\n",
    "# Création d'un DataFrame composé de deux variables : label et features\n",
    "df_ml = spark.createDataFrame(rdd_ml, ['label', 'features'])\n",
    "\n",
    "# Affichage des 10 premières lignes du DataFrame\n",
    "df_ml.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87fc1ba-d925-494f-a62e-197a4a4121fd",
   "metadata": {},
   "source": [
    "Afin d'évaluer les performances du modèle de régression, il faut mettre de côté une partie des données qui attesteront de la qualité du modèle une fois entraîné. Pour cela, il faut systématiquement diviser les données en un ensemble d'entraînement et un ensemble de test.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "Usuellement, la taille de jeu de test est comprise entre 15% et 30% de la quantité totale de données disponibles. Le choix de la répartition dépend essentiellement de la quantité et de la qualité des données disponibles.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "La méthode randomSplit permet de séparer un DataFrame en deux. Par exemple, la création d'un DataFrame d'entraînement contenant 70% des données et un de test en contenant 30%, se fait de la façon suivante :<br>\n",
    "train, test = df.randomSplit([.7, .3], seed= 222)\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "Imposer un seed sert simplement à rendre les résultats reproductibles.\n",
    "</div>\n",
    "\n",
    "* __(e)__ Créer deux DataFrames appelés train et test contenant respectivement 80% et 20% des données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64373e81-1b2b-4cc2-8042-e6d28237e274",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = df_ml.randomSplit([.8, .2], seed= 222)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f487f91f-bac1-410e-851c-96d2b36ec0e4",
   "metadata": {},
   "source": [
    "## 3. Régression linéaire"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93a6a79-74ec-4997-aa8c-224122e83e2b",
   "metadata": {},
   "source": [
    "Spark ML contient de nombreuses fonctions de Machine Learning, commençons ici par la plus basique : la régression linéaire. Elle est présente sous le nom LinearRegression (https://spark.apache.org/docs/latest/ml-classification-regression.html#linear-regression) dans le module pyspark.ml.regression. Cette fonction permet d'effectuer une régression linéaire de façon distribuée et effectue les calculs sur les différents clusters prédéfinis dans la SparkSession, quel que soit leur nombre ou la taille de la base de données.\n",
    "\n",
    "Pour l'utiliser, il faut procéder avec les deux étapes habituelles :\n",
    "* Créer la fonction avec les paramètres spécifiques au contexte.\n",
    "* Utiliser la méthode fit pour l'appliquer aux données.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "Cliquez ici (https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.regression.LinearRegression.html#pyspark.ml.regression.LinearRegression) pour plus d'informations sur les différents paramètres disponibles de LinearRegression.\n",
    "</div>\n",
    "\n",
    "__Exemple :__\n",
    "\n",
    "```python\n",
    "lr = LinearRegression(labelCol='label', featuresCol= 'features', maxIter=10, regParam=0.3)\n",
    "```\n",
    "\n",
    "* __(a)__ Importer la fonction LinearRegression du sous-module pyspark.ml.regression.\n",
    "* __(b)__ Créer lr, une fonction de régression linéaire distribuée pour l'appliquer à l'ensemble train.\n",
    "* __(c)__ Créer linearModel, le modèle issu de lr appliqué à train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143d45c8-85f8-401b-a4f5-cbb3f79b0ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import de LinearRegression du package pyspark.ml.regression\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# Création d'une fonction de régression linéaire\n",
    "lr = LinearRegression(labelCol='label', featuresCol= 'features')\n",
    "\n",
    "# Apprentissage sur les données d'entraînement \n",
    "linearModel = lr.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6957afb7-2023-4c50-a897-063fb6e318fc",
   "metadata": {},
   "source": [
    "Maintenant que le modèle d'apprentissage est construit, il est possible d'effectuer des prédictions sur les données test.\n",
    "\n",
    "Pour cela, les modèles Spark ML possèdent une méthode transform() permettant d'effectuer des prédictions en prenant en seul argument la base de données de test.\n",
    "\n",
    "* __(d)__ Créer un DataFrame predicted contenant les données prédites et les labels correspondants.\n",
    "* __(e)__ Afficher un extrait de predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943ff9b4-3c40-430f-b1f2-8f6510bdeb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul des prédictions sur les données test\n",
    "predicted = linearModel.transform(test)\n",
    "\n",
    "# Affichage des prédictions\n",
    "predicted.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b18c18-3be4-4ad8-a344-a0b9f00c0415",
   "metadata": {},
   "source": [
    "## 4. Evaluation du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e5ed9e-02c8-4357-9fe6-cd97976a9d2a",
   "metadata": {},
   "source": [
    "Afin d'évaluer la qualité du modèle, il est possible de chercher les informations au sein de l'attribut summary de notre modèle.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "Taper linearModel.summary. puis appuyez sur Tab pour faire apparaître tous les différents attributs du summary\n",
    "</div>\n",
    "\n",
    "* __(a)__ Calculer et afficher le RMSE (Root Mean Squared Error) de la régression.\n",
    "* __(b)__ Calculer et afficher le R 22  de la régression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28365550-390a-43da-89c7-894a2bdc03f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul et affichage du RMSE\n",
    "print(\"RMSE:\", linearModel.summary.rootMeanSquaredError)\n",
    "\n",
    "# Calcul et affichage du R2\n",
    "print(\"R2:  \", linearModel.summary.r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e074a721-9e4c-4f79-8e45-94e890c03d3a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "Bien que la RMSE indique une erreur moyenne relativement faible (inférieur à 10), le R 22  reste très faible (inférieur à 0.25). On peut donc penser que le modèle de régression est dans ce cas un mauvais indicateur de la décennie dans laquelle la chanson a été composée.\n",
    "</div>\n",
    "\n",
    "Une fois les résultats obtenus, il est possible d'optimiser le modèle par rapport à ces mesures. De façon générale, l'optimisation des modèles se fait en modifiant les variables explicatives utilisées, en utilisant un autre modèle de régression et/ou en changeant les paramètres du modèle grâce à la documentation de PySpark.\n",
    "\n",
    "Une fois que le modèle prédit adéquatement l'année de sortie, Spark ML offre la possibilité de récupérer les coefficients grâce aux attributs coefficients et intercept du modèle.\n",
    "\n",
    "* __(c)__ Afficher les coefficients coefficients du modèle. La fonction pprint du module pprint permet d'avoir un affichage plus élégant des données.\n",
    "* __(d)__ Fermer la session spark en utilisant la méthode stop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5af07c-b4ff-40c5-a3b4-8e87456b3acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# Affichage des Coefficients du modèle linéaire\n",
    "pprint(linearModel.coefficients)\n",
    "\n",
    "# Fermeture de la session Spark \n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3596451-a649-4fea-a10c-a37afddb3dc4",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "Maintenant que vous avez appris à programmer une régression linéaire en utilisant Spark ML, vous n'êtes qu'à quelques pas de maîtriser tout algorithme de régression distribué sous Spark. Pour vous aider à retenir l'essentiel, en voici un aperçu :<br>\n",
    "• 1. Transformer la base de données en format svmlib :<br>\n",
    "  • Sélectionner les variables numériques à utiliser pour la régression.<br>\n",
    "  • Placer la variable à expliquer en première position.<br>\n",
    "  • Mapper un couple (label, vecteur de features) dans un RDD.<br>\n",
    "  • Convertir ce RDD en DataFrame et nommer les variables 'label' et 'features'.<br>\n",
    "• 2. Séparer la base de données en deux échantillons train et test.<br>\n",
    "• 3. Appliquer un modèle de classification.<br>\n",
    "• 4. Evaluer le modèle.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "Spark est en constante amélioration et possède aujourd'hui quelques régresseurs notables. Ils sont utilisables de la même façon en important ces fonction depuis pyspark.ml.regression. Vous êtes invité à consulter la documentation pour observer les différents paramètres à prendre en compte pour optimiser ces algorithmes :<br>\n",
    "  • LinearRegression() pour effectuer une régression linéaire lorsque le label est présupposé suivre une loi normale.<br>\n",
    "  • GeneralizedLinearRegression() pour effectuer une régression linéaire généralisée lorsque le label est présupposé suivre une autre loi que l'on spécifie dans le paramètre family (gaussian, binomial, poisson, gamma).<br>\n",
    "  • AFTSurvivalRegression() pour effectuer une analyse de survie.<br><br>\n",
    "\n",
    "Il est également possible d'utiliser les algorithmes, qui gèrent également les variables catégorielles, détaillés dans l'exercice suivant :<br>\n",
    "  • DecisionTreeRegressor() pour un arbre de décision.<br>\n",
    "  • RandomForestRegressor() pour une forêt aléatoire d'arbres de décision.<br>\n",
    "  • GBTRegressor() pour une forêt d'arbres gradient-boosted.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7943d43-3b22-467b-9970-639ffa45cd0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
